{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"53_autoencoder.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"KQYxMuvvsuA_","colab_type":"text"},"cell_type":"markdown","source":["# Auto-Encoder Example\n","\n","Build a 2 layers auto-encoder with TensorFlow to compress images to a lower latent space and then reconstruct them.\n","\n","- Author: Aymeric Damien\n","- Project: https://github.com/aymericdamien/TensorFlow-Examples/"]},{"metadata":{"id":"LewHkelBsuBB","colab_type":"text"},"cell_type":"markdown","source":["## Auto-Encoder Overview\n","\n","<img src=\"http://kvfrans.com/content/images/2016/08/autoenc.jpg\" alt=\"ae\" style=\"width: 800px;\"/>\n","\n","References:\n","- [Gradient-based learning applied to document recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n","\n","## MNIST Dataset Overview\n","\n","This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n","\n","![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n","\n","More info: http://yann.lecun.com/exdb/mnist/"]},{"metadata":{"id":"z-I-KVCXsuBB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["from __future__ import division, print_function, absolute_import\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QmV21rdHsuBG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Import MNIST data\n","mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YjVgEvIysuBI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Graph Clear\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zxPJJyYzsuBL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Training Parameters\n","learning_rate = 0.01\n","\n","# tf Graph input (only pictures)\n","X = tf.placeholder(\"float\", [None, 784])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8lO4BGn4suBO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Building the encoder\n","def encoder(x):\n","    # Encoder Hidden layer with sigmoid activation #1\n","    we1 = tf.Variable(tf.random_normal([784, 256]))\n","    be1 = tf.Variable(tf.random_normal([256]))\n","    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, we1), be1))\n","\n","    # Encoder Hidden layer with sigmoid activation #2\n","    we2 = tf.Variable(tf.random_normal([256, 128]))\n","    be2 = tf.Variable(tf.random_normal([128]))\n","    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, we2), be2))\n","\n","    return layer_2\n","\n","\n","# Building the decoder\n","def decoder(x):\n","    # Decoder Hidden layer with sigmoid activation #1\n","    wd1 = tf.Variable(tf.random_normal([128, 256]))\n","    bd1 = tf.Variable(tf.random_normal([256]))\n","    layer_1 = tf.nn.relu(tf.add(tf.matmul(x, wd1), bd1))\n","    \n","    # Decoder Hidden layer with sigmoid activation #2\n","    wd2 = tf.Variable(tf.random_normal([256, 784]))\n","    bd2 = tf.Variable(tf.random_normal([784]))\n","    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, wd2), bd2))\n","\n","    return layer_2\n","\n","# Construct model\n","encoder_op = encoder(X)\n","decoder_op = decoder(encoder_op)\n","\n","# Define loss and optimizer, minimize the squared error\n","loss = tf.reduce_mean(tf.pow(X - decoder_op, 2))\n","optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gX_jS9iMsuBR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["epochs = 20\n","batch_size = 64\n","display_step = int(5000 / batch_size)\n","num_batches = int(mnist.train.num_examples / batch_size)\n","\n","# Start Training\n","# Start a new TF session\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","for e in range(epochs):\n","    print('epoch %d' % (e + 1))\n","    \n","    # Training\n","    for i in range(num_batches):\n","        # Prepare Data\n","        # Get the next batch of MNIST data (only images are needed, not labels)\n","        batch_x, _ = mnist.train.next_batch(batch_size)\n","\n","        # Run optimization op (backprop) and cost op (to get loss value)\n","        sess.run(optimizer, feed_dict={X: batch_x})\n","        # Display logs per step\n","        if ((i + 1) % display_step == 0):\n","            l = sess.run(loss, feed_dict={X: batch_x})\n","            print('Step %i: Minibatch Loss: %f' % ((i + 1) * batch_size, l))\n","            \n","\n","l = sess.run(loss, feed_dict={X: mnist.test.images})\n","print('Test set Loss: %f' % l)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"heUpS_9_suBU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Testing\n","# Encode and decode images from test set and visualize their reconstruction.\n","n = 4\n","canvas_orig = np.empty((28 * n, 28 * n))\n","canvas_recon = np.empty((28 * n, 28 * n))\n","for i in range(n):\n","    # MNIST test set\n","    batch_x, _ = mnist.test.next_batch(n)\n","    # Encode and decode the digit image\n","    g = sess.run(decoder_op, feed_dict={X: batch_x})\n","    \n","    # Display original images\n","    for j in range(n):\n","        # Draw the generated digits\n","        canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = batch_x[j].reshape([28, 28])\n","    # Display reconstructed images\n","    for j in range(n):\n","        # Draw the generated digits\n","        canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n","\n","print(\"Original Images\")     \n","plt.figure(figsize=(n, n))\n","plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n","plt.show()\n","\n","print(\"Reconstructed Images\")\n","plt.figure(figsize=(n, n))\n","plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o3ybLcg5suBY","colab_type":"text"},"cell_type":"markdown","source":["### 모델에 fashion mnist를 넣고 돌리면?"]},{"metadata":{"id":"oZOZphLZsuBZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["'''\n","트레이닝 데이터로 활용할 60000개 이미지, 테스트용 10000개 이미지\n","28*28 사이즈의 이미지가 그냥 주욱 784칸 짜리 어레이에 담겨있다.\n","test, traing 데이터가 분리되어 있다.\n","레이블은 one-hot encoding을 해주어야 한다. \n","'''\n","\n","(train_x, train_y), (test_x, test_y) = tf.keras.datasets.fashion_mnist.load_data()\n","\n","print(train_x.shape)\n","print(train_y.shape)\n","print(test_x.shape)\n","print(test_y.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l6yqv6m9suBb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Testing fashion\n","# Encode and decode images from test set and visualize their reconstruction.\n","n = 4\n","canvas_orig = np.empty((28 * n, 28 * n))\n","canvas_recon = np.empty((28 * n, 28 * n))\n","for i in range(n):\n","    # MNIST test set\n","    batch_x = train_x[:n].reshape(-1, 784)\n","    # Encode and decode the digit image\n","    g = sess.run(decoder_op, feed_dict={X: batch_x})\n","    \n","    # Display original images\n","    for j in range(n):\n","        # Draw the generated digits\n","        canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = batch_x[j].reshape([28, 28])\n","    # Display reconstructed images\n","    for j in range(n):\n","        # Draw the generated digits\n","        canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n","\n","print(\"Original Images\")     \n","plt.figure(figsize=(n, n))\n","plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n","plt.show()\n","\n","print(\"Reconstructed Images\")\n","plt.figure(figsize=(n, n))\n","plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KetEB8FRsuBf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}