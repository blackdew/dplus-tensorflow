{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"52_CNN - Models.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":["HWFU61ce5UoF","oXLEy5yRVarT","Mge3PABGv22v","FULCrLu0WmIh"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"M1wHvbodVaqm","colab_type":"text"},"cell_type":"markdown","source":["# 52. CNN - Models\n","\n","<p style=\"text-align: right;\">\n","blackdew7@gmail.com<br>\n","Your name :\n","</p>\n","\n","#### 선행지식\n","1. TensorFlow 다루기 기초\n","2. 모델링을 한다는 것에 대한 이해.\n","3. Supervised Learning 중 Classification에 대한 기본 지식.\n","4. CNN에 대한 기본 구조와 개념\n","\n","#### 실습목표\n","1. AlexNet을 구현해본다. \n","2. VGGNet을 구현해본다. \n","3. GoogLeNet을 구현해본다. \n","4. ResNet을 구현해본다. \n","\n","#### 사용데이터.\n","1. The CIFAR-10 dataset : https://www.cs.toronto.edu/~kriz/cifar.html\n","\n","![image](https://github.com/blackdew/dplus-tensorflow/blob/master/img/06_network_flowchart.png?raw=true)"]},{"metadata":{"id":"vIZ7vSyf5hKQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# gpu 사용확인 \n","\n","import tensorflow as tf\n","tf.test.gpu_device_name()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cHbbuVFEhLi7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","\n","# cifar10\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n","\n","train_y = train_y.reshape(-1,)\n","test_y = test_y.reshape(-1,)\n","\n","'''\n","트레이닝 데이터로 활용할 50000개 이미지, 테스트용 10000개 이미지\n","32 * 32 * 3 사이즈의 이미지가 어레이에 담겨있다.\n","레이블은 one-hot encoding이 되어 있지 않다.\n","'''\n","\n","print(train_x.shape)\n","print(train_y.shape)\n","print(test_x.shape)\n","print(test_y.shape)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tncZlFEVhMfo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","import matplotlib.pyplot as plt\n","\n","class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","# 이미지 출력을 위한 함수\n","def cifar10_plot(i):\n","    pixels = test_x[i]\n","    a = int(test_y[i])\n","    plt.title('Label: {} ({})'.format(class_names[a], a))\n","    plt.imshow(pixels, cmap='gray')\n","    return plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ho8CXclahztX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["cifar10_plot(np.random.randint(0, high=9999))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HWFU61ce5UoF","colab_type":"text"},"cell_type":"markdown","source":["\n","## CIFAR-10 + AlexNet\n","\n","![image](http://mblogthumb1.phinf.naver.net/20160314_204/laonple_14579300603930fQ7q_PNG/%C0%CC%B9%CC%C1%F6_2.png?type=w2)\n"]},{"metadata":{"id":"BLkSP3-2CWtI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","\n","# Graph Clear\n","tf.reset_default_graph()\n","tf.set_random_seed(2017) # random seeding - reproduct\n","\n","############################\n","# Place Holders\n","lr = tf.placeholder(tf.float32) # learning rate\n","kp = tf.placeholder(tf.float32) # dropout\n","\n","X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"X\")\n","Y = tf.placeholder(tf.float32, [None, 10], name=\"Y\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bJ0aHrUFCZm-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 01-1 / (11, 11, 3, 48) but, (3, 3, 3, 8)\n","\n","# for Conv Layer 01 filter \n","w11 = tf.Variable(tf.random_normal([3, 3, 3, 8], stddev=0.01))\n","b11 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","tf.add_to_collection('weights', w11)\n","\n","# Convolution Layer -> (?, 32, 32, 8)\n","conv11 = tf.add(tf.nn.conv2d(X, w11, strides=[1, 1, 1, 1], padding='SAME'), b11)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv11, [0])\n","conv11 = tf.nn.batch_normalization(conv11, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv11 = tf.nn.relu(conv11)\n","\n","# Pooling Layer -> (?, 16, 16, 8)\n","pool11 = tf.nn.max_pool(conv11, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","#########################\n","# ConvLayer 01-2 / (5, 5, 48, 128) but, (3, 3, 8, 16)\n","\n","# for Conv Layer 02 filter\n","w12 = tf.Variable(tf.random_normal([3, 3, 8, 16], stddev=0.01))\n","b12 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","tf.add_to_collection('weights', w12)\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv12 = tf.add(tf.nn.conv2d(pool11, w12, strides=[1, 1, 1, 1], padding='SAME'), b12)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv12, [0])\n","conv12 = tf.nn.batch_normalization(conv12, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv12 = tf.nn.relu(conv12)\n","\n","# Pooling Layer -> (?, 8, 8, 16)\n","pool12 = tf.nn.max_pool(conv12, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8OXdAFJaCbrp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 02-1 / (11, 11, 3, 48) but, (3, 3, 3, 8)\n","\n","# for Conv Layer 01 filter\n","w21 = tf.Variable(tf.random_normal([3, 3, 3, 8], stddev=0.01))\n","b21 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","tf.add_to_collection('weights', w21)\n","\n","# Convolution Layer -> (?, 32, 32, 8)\n","conv21 = tf.add(tf.nn.conv2d(X, w21, strides=[1, 1, 1, 1], padding='SAME'), b21)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv21, [0])\n","conv21 = tf.nn.batch_normalization(conv21, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv21 = tf.nn.relu(conv21)\n","\n","# Pooling Layer -> (?, 16, 16, 8)\n","pool21 = tf.nn.max_pool(conv21, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","#########################\n","# ConvLayer 02-2 / (5, 5, 48, 128) but, (3, 3, 8, 16)\n","\n","# for Conv Layer 02 filter\n","w22 = tf.Variable(tf.random_normal([3, 3, 8, 16], stddev=0.01))\n","b22 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","tf.add_to_collection('weights', w22)\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv22 = tf.add(tf.nn.conv2d(pool21, w22, strides=[1, 1, 1, 1], padding='SAME'), b22)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv22, [0])\n","conv22 = tf.nn.batch_normalization(conv22, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv22 = tf.nn.relu(conv22)\n","\n","# Pooling Layer -> (?, 8, 8, 16)\n","pool22 = tf.nn.max_pool(conv22, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mMTS4EufCdt9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 01-3\n","# p21 * (3, 3, 128, 192) + p22 * (3, 3, 128, 192). but\n","# p21 * (3, 3, 16, 24) + p22 * (3, 3, 16, 24)\n","\n","# for Conv Layer 03 filter\n","w131 = tf.Variable(tf.random_normal([3, 3, 16, 24], stddev=0.01))\n","b131 = tf.Variable(tf.random_normal([24], stddev=0.01))\n","tf.add_to_collection('weights', w131)\n","\n","# Convolution Layer -> (?, 8, 8, 24)\n","conv131 = tf.add(tf.nn.conv2d(pool12, w131, strides=[1, 1, 1, 1], padding='SAME'), b131)\n","\n","# for Conv Layer 03 filter\n","w132 = tf.Variable(tf.random_normal([3, 3, 16, 24], stddev=0.01))\n","b132 = tf.Variable(tf.random_normal([24], stddev=0.01))\n","tf.add_to_collection('weights', w132)\n","\n","# Convolution Layer -> (?, 8, 8, 24)\n","conv132 = tf.add(tf.nn.conv2d(pool22, w132, strides=[1, 1, 1, 1], padding='SAME'), b132)\n","\n","# convolution result merge -> (?, 8, 8, 24)\n","conv13 = tf.add(conv131, conv132)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv13, [0])\n","conv13 = tf.nn.batch_normalization(conv13, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv13 = tf.nn.relu(conv13)\n","\n","\n","#########################\n","# ConvLayer 01-4 / (3, 3, 192, 192), but (3, 3, 24, 24)\n","\n","# for Conv Layer 04 filter\n","w14 = tf.Variable(tf.random_normal([3, 3, 24, 24], stddev=0.01))\n","b14 = tf.Variable(tf.random_normal([24], stddev=0.01))\n","tf.add_to_collection('weights', w14)\n","\n","# Convolution Layer -> (?, 8, 8, 24)\n","conv14 = tf.add(tf.nn.conv2d(conv13, w14, strides=[1, 1, 1, 1], padding='SAME'), b14)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv14, [0])\n","conv14 = tf.nn.batch_normalization(conv14, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv14 = tf.nn.relu(conv14)\n","    \n","    \n","#########################\n","# ConvLayer 01-5 / (3, 3, 192, 128), but (3, 3, 24, 16)\n","\n","# for Conv Layer 05 filter\n","w15 = tf.Variable(tf.random_normal([3, 3, 24, 16], stddev=0.01))\n","b15 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","tf.add_to_collection('weights', w15)\n","\n","# Convolution Layer -> (?, 8, 8, 16)\n","conv15 = tf.add(tf.nn.conv2d(conv14, w15, strides=[1, 1, 1, 1], padding='SAME'), b15)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv15, [0])\n","conv15 = tf.nn.batch_normalization(conv15, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv15 = tf.nn.relu(conv15)\n","\n","# Pooling Layer -> (?, 4, 4, 16)\n","pool15 = tf.nn.max_pool(conv15, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DibK3w4uCg9u","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 02-3\n","# p21 * (3, 3, 128, 192) + p22 * (3, 3, 128, 192). but\n","# p21 * (3, 3, 16, 24) + p22 * (3, 3, 16, 24)\n","\n","# for Conv Layer 03 filter\n","w231 = tf.Variable(tf.random_normal([3, 3, 16, 24], stddev=0.01))\n","b231 = tf.Variable(tf.random_normal([24], stddev=0.01))\n","tf.add_to_collection('weights', w231)\n","\n","# Convolution Layer -> (?, 8, 8, 24)\n","conv231 = tf.add(tf.nn.conv2d(pool12, w231, strides=[1, 1, 1, 1], padding='SAME'), b231)\n","\n","# for Conv Layer 03 filter\n","w232 = tf.Variable(tf.random_normal([3, 3, 16, 24], stddev=0.01))\n","b232 = tf.Variable(tf.random_normal([24], stddev=0.01))\n","tf.add_to_collection('weights', w232)\n","\n","# Convolution Layer -> (?, 8, 8, 24)\n","conv232 = tf.add(tf.nn.conv2d(pool22, w232, strides=[1, 1, 1, 1], padding='SAME'), b232)\n","\n","# convolution result merge -> (?, 8, 8, 24)\n","conv23 = tf.add(conv231, conv232)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv23, [0])\n","conv23 = tf.nn.batch_normalization(conv23, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv23 = tf.nn.relu(conv23)\n","\n","\n","#########################\n","# ConvLayer 02-4 / (3, 3, 192, 192), but (3, 3, 24, 24)\n","\n","# for Conv Layer 04 filter\n","w24 = tf.Variable(tf.random_normal([3, 3, 24, 24], stddev=0.01))\n","b24 = tf.Variable(tf.random_normal([24], stddev=0.01))\n","tf.add_to_collection('weights', w24)\n","\n","# Convolution Layer -> (?, 8, 8, 24)\n","conv24 = tf.add(tf.nn.conv2d(conv23, w24, strides=[1, 1, 1, 1], padding='SAME'), b24)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv24, [0])\n","conv24 = tf.nn.batch_normalization(conv24, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv24 = tf.nn.relu(conv24)\n","    \n","    \n","#########################\n","# ConvLayer 01-5 / (3, 3, 192, 128), but (3, 3, 24, 16)\n","\n","# for Conv Layer 05 filter\n","w25 = tf.Variable(tf.random_normal([3, 3, 24, 16], stddev=0.01))\n","b25 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","tf.add_to_collection('weights', w25)\n","\n","# Convolution Layer -> (?, 8, 8, 16)\n","conv25 = tf.add(tf.nn.conv2d(conv24, w25, strides=[1, 1, 1, 1], padding='SAME'), b25)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv25, [0])\n","conv25 = tf.nn.batch_normalization(conv25, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv25 = tf.nn.relu(conv25)\n","\n","# Pooling Layer -> (?, 4, 4, 16)\n","pool25 = tf.nn.max_pool(conv25, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fn6pIazB8iTe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["###########################################\n","# Flatten layer & Fully Connected Layer 01-1 / (13 * 13 * 256, 4096) but (4 * 4 * 32, 256)\n","\n","###############################\n","# Flatten layer\n","pool5 = tf.concat([pool15, pool25], axis=3)\n","\n","dim = pool5.get_shape().as_list()\n","flat_dim = dim[1] * dim[2] * dim[3] # 4 * 4 * 32\n","flat = tf.reshape(pool5, [-1, flat_dim])\n","\n","###############################\n","# Fully Connected Layer 06\n","\n","# for Final FC 4x4x128 inputs\n","wfc1 = tf.Variable(tf.random_normal([flat_dim, 256], stddev=0.01))\n","bfc1 = tf.Variable(tf.random_normal([256]))\n","tf.add_to_collection('weights', wfc1)\n","\n","# fc layer\n","fc1 = tf.add(tf.matmul(flat, wfc1), bfc1)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(fc1, [0])\n","fc1 = tf.nn.batch_normalization(fc1, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","fc1 = tf.nn.relu(fc1)\n","# dropout\n","fc1 = tf.nn.dropout(fc1, kp)\n","\n","    \n","###############################\n","# Fully Connected Layer 07\n","\n","# for Final FC 1024 input\n","wfc2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))\n","bfc2 = tf.Variable(tf.random_normal([256]))\n","tf.add_to_collection('weights', wfc2)\n","\n","\n","# fc layer\n","fc2 = tf.add(tf.matmul(fc1, wfc2), wfc2)\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(fc2, [0])\n","fc2 = tf.nn.batch_normalization(fc2, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","fc2 = tf.nn.relu(fc2)\n","# dropout\n","fc2 = tf.nn.dropout(fc2, kp)\n","\n","###############################\n","# Fully Connected Layer 08\n","\n","# for Final FC 1000 inputs -> 10 outputs\n","wfc3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n","bfc3 = tf.Variable(tf.random_normal([10]))\n","tf.add_to_collection('weights', wfc3)\n","\n","# fc layer\n","logits = tf.add(tf.matmul(fc2, wfc3), bfc3)\n","\n","#########################\n","# Cost & Optimizer\n","\n","# Cost(loss) function & Optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y), name=\"cost\")\n","\n","l2_loss = tf.reduce_sum(0.00005 * tf.stack([tf.nn.l2_loss(v) for v in tf.get_collection('weights')])) # LAMBDA = 0.00005 # for weight decay\n","\n","loss = tf.add(cost, l2_loss)\n","\n","optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n","\n","\n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n","acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YLTC1kZZVarE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# Session initialize\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","###########################\n","# Traning\n","\n","print('Learning started. It takes sometime.')\n","\n","epochs = 20\n","batch_size = 256\n","learning_rate = 0.01\n","dropout = 0.7\n","n_of_batches = int(len(train_x) / batch_size)\n","\n","for epoch in range(epochs): \n","    print(\"%dth epoch\" % (epoch + 1))\n","\n","    for i in range(n_of_batches):\n","\n","        X_batch = train_x[(i * batch_size):((i + 1) * batch_size)]\n","        Y_batch = train_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","        sess.run(optimizer, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                       lr: learning_rate, kp: dropout})\n","\n","        # 학습 상황 디스플레이\n","        if ((i + 1) % 10 == 0): \n","            loss, accuracy = sess.run([cost, acc], \n","                                      feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                                 lr: learning_rate, kp: 1.0})\n","            print(\"%dth records, training cost: %.3f, accuracy: %.2f\" % ((i + 1) * batch_size, loss, accuracy * 100))\n","\n","print(\"Training Complete\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IZd2fP2bLZh-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["###########################\n","# Evaluation\n","# accuracy = sess.run(acc, feed_dict={X: test_x, Y: np.eye(10)[test_y], lr: learning_rate, kp: 1.0})\n","# print('Accuracy: %.2f' % (accuracy * 100))\n","\n","n_of_batches = int(len(test_x) / batch_size)\n","accuracy = 0\n","for i in range(n_of_batches):\n","\n","    X_batch = test_x[(i * batch_size):((i + 1) * batch_size)]\n","    Y_batch = test_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","    accuracy += sess.run(acc, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], lr: learning_rate, kp: 1.0})\n","    \n","print('Accuracy: %.2f' % ((accuracy / n_of_batches) * 100))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oXLEy5yRVarT","colab_type":"text"},"cell_type":"markdown","source":["## CIFAR-10 + VGG16\n","\n","\n","![image](https://adeshpande3.github.io/assets/VGGNet.png)\n"]},{"metadata":{"id":"CAQeFT_kVarW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","\n","# Graph Clear\n","tf.reset_default_graph()\n","tf.set_random_seed(2017) # random seeding - reproduct\n","\n","############################\n","# Place Holders\n","lr = tf.placeholder(tf.float32) # learning rate\n","kp = tf.placeholder(tf.float32) # dropout\n","\n","X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"X\")\n","Y = tf.placeholder(tf.float32, [None, 10], name=\"Y\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CRSv893EJ-d5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 1, 2 (원본 인풋: 224 * 224 * 3)\n","\n","# for Conv Layer 01 filter / (3, 3, 3, 64) but (3, 3, 3, 16)\n","w1 = tf.Variable(tf.random_normal([3, 3, 3, 16], stddev=0.01))\n","b1 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 32, 32, 16)\n","conv1 = tf.add(tf.nn.conv2d(X, w1, strides=[1, 1, 1, 1], padding='SAME'), b1)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv1, [0])\n","conv1 = tf.nn.batch_normalization(conv1, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv1 = tf.nn.relu(conv1)\n","\n","# for Conv Layer 02 filter / (3, 3, 64, 64) but (3, 3, 16, 16)\n","w2 = tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=0.01))\n","b2 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 32, 32, 16)\n","conv2 = tf.add(tf.nn.conv2d(conv1, w2, strides=[1, 1, 1, 1], padding='SAME'), b2)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv2, [0])\n","conv2 = tf.nn.batch_normalization(conv2, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv2 = tf.nn.relu(conv2)\n","\n","# Pooling Layer -> (?, 16, 16, 16)\n","pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WjeurXeaKA7x","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 3, 4 (원래 인풋: 112 * 112 * 64)\n","\n","# for Conv Layer 03 filter / (3, 3, 64, 128) but (3, 3, 16, 32)\n","w3 = tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=0.01))\n","b3 = tf.Variable(tf.random_normal([32], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 32)\n","conv3 = tf.add(tf.nn.conv2d(pool2, w3, strides=[1, 1, 1, 1], padding='SAME'), b3)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv3, [0])\n","conv3 = tf.nn.batch_normalization(conv3, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv3 = tf.nn.relu(conv3)\n","\n","# for Conv Layer 04 filter / (3, 3, 128, 128) but (3, 3, 32, 32)\n","w4 = tf.Variable(tf.random_normal([3, 3, 32, 32], stddev=0.01))\n","b4 = tf.Variable(tf.random_normal([32], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 32)\n","conv4 = tf.add(tf.nn.conv2d(conv3, w4, strides=[1, 1, 1, 1], padding='SAME'), b4)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv4, [0])\n","conv4 = tf.nn.batch_normalization(conv4, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv4 = tf.nn.relu(conv4)\n","\n","# Pooling Layer -> (?, 8, 8, 32)\n","pool4 = tf.nn.max_pool(conv4, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DgqGFw46KkCw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 5, 6, 7 (원래 인풋: 56 * 56 * 128)\n","\n","# for Conv Layer 05 filter / (3, 3, 128, 256) but (3, 3, 32, 64)\n","w5 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n","b5 = tf.Variable(tf.random_normal([64], stddev=0.01))\n","\n","# Convolution Layer -> (?, 8, 8, 64)\n","conv5 = tf.add(tf.nn.conv2d(pool4, w5, strides=[1, 1, 1, 1], padding='SAME'), b5)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv5, [0])\n","conv5 = tf.nn.batch_normalization(conv5, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv5 = tf.nn.relu(conv5)\n","\n","# for Conv Layer 06 filter / (3, 3, 256, 256) but (3, 3, 64, 64)\n","w6 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n","b6 = tf.Variable(tf.random_normal([64], stddev=0.01))\n","\n","# Convolution Layer 02 -> (?, 8, 8, 64)\n","conv6 = tf.add(tf.nn.conv2d(conv5, w6, strides=[1, 1, 1, 1], padding='SAME'), b6)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv6, [0])\n","conv6 = tf.nn.batch_normalization(conv6, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv6 = tf.nn.relu(conv6)\n","\n","# for Conv Layer 07 filter / (3, 3, 256, 256) but (3, 3, 64, 64)\n","w7 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n","b7 = tf.Variable(tf.random_normal([64], stddev=0.01))\n","\n","# Convolution Layer -> (?, 8, 8, 64)\n","conv7 = tf.add(tf.nn.conv2d(conv6, w7, strides=[1, 1, 1, 1], padding='SAME'), b7)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv7, [0])\n","conv7 = tf.nn.batch_normalization(conv7, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv7 = tf.nn.relu(conv7)\n","\n","# Pooling Layer -> (?, 4, 4, 64)\n","pool7 = tf.nn.max_pool(conv7, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","##############################################################################################################################\n","# 32x32 CIFAR-10 이미지의 경우, 더 진행하면 max pooling으로 인해 차원이 너무 축소되어 버려 의미가 없음.\n","\n","#########################\n","# ConvLayer 8, 9, 10 (원래 인풋: 28 * 28 * 256)\n","\n","# # for Conv Layer 08 filter / (3, 3, 256, 512)\n","# w8 = tf.Variable(tf.random_normal([3, 3, 256, 512], stddev=0.01))\n","# b8 = tf.Variable(tf.random_normal([512], stddev=0.01))\n","\n","# # Convolution Layer -> (?, 28, 28, 512)\n","# conv8 = tf.add(tf.nn.conv2d(pool7, w8, strides=[1, 1, 1, 1], padding='SAME'), b8)\n","\n","# # relu\n","# conv8 = tf.nn.relu(conv8)\n","\n","# # for Conv Layer 09 filter / (3, 3, 512, 512)\n","# w9 = tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.01))\n","# b9 = tf.Variable(tf.random_normal([512], stddev=0.01))\n","\n","# # Convolution Layer -> (?, 28, 28, 512)\n","# conv9 = tf.add(tf.nn.conv2d(conv8, w9, strides=[1, 1, 1, 1], padding='SAME'), b9)\n","\n","# # relu\n","# conv9 = tf.nn.relu(conv9)\n","\n","# # for Conv Layer 10 filter / (3, 3, 512, 512)\n","# w10 = tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.01))\n","# b10 = tf.Variable(tf.random_normal([512], stddev=0.01))\n","\n","# # Convolution Layer -> (?, 28, 28, 512)\n","# conv10 = tf.add(tf.nn.conv2d(conv9, w10, strides=[1, 1, 1, 1], padding='SAME'), b10)\n","\n","# # relu\n","# conv10 = tf.nn.relu(conv10)\n","\n","# # Pooling Layer -> (?, 14, 14, 512)\n","# pool10 = tf.nn.max_pool(conv10, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n","\n","\n","# #########################\n","# # ConvLayer 11, 12, 13 (원래 인풋: 14 * 14 * 512)\n","\n","# # for Conv Layer 11 filter / (3, 3, 512, 512)\n","# w11 = tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.01))\n","# b11 = tf.Variable(tf.random_normal([512], stddev=0.01))\n","\n","# # Convolution Layer -> (?, 14, 14, 512)\n","# conv11 = tf.add(tf.nn.conv2d(conv10, w11, strides=[1, 1, 1, 1], padding='SAME'), b11)\n","\n","# # relu\n","# conv11 = tf.nn.relu(conv11)\n","\n","# # for Conv Layer 12 filter / (3, 3, 512, 512)\n","# w12 = tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.01))\n","# b12 = tf.Variable(tf.random_normal([512], stddev=0.01))\n","\n","# # Convolution Layer -> (?, 14, 14, 512)\n","# conv12 = tf.add(tf.nn.conv2d(conv11, w12, strides=[1, 1, 1, 1], padding='SAME'), b12)\n","\n","# # relu\n","# conv12 = tf.nn.relu(conv12)\n","\n","# # for Conv Layer 13 filter / (3, 3, 512, 512)\n","# w13 = tf.Variable(tf.random_normal([3, 3, 512, 512], stddev=0.01))\n","# b13 = tf.Variable(tf.random_normal([512], stddev=0.01))\n","\n","# # Convolution Layer -> (?, 14, 14, 512)\n","# conv13 = tf.add(tf.nn.conv2d(conv12, w13, strides=[1, 1, 1, 1], padding='SAME'), b13)\n","\n","# # relu\n","# conv13 = tf.nn.relu(conv13)\n","\n","# # Pooling Layer -> (?, 7, 7, 512)\n","# pool13 = tf.nn.max_pool(conv13, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_nwrVsqMVarb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["##################################################\n","# Flatten layer & Fully Connected Layer\n","\n","###############################\n","# Flatten layer\n","dim = pool7.get_shape().as_list()\n","flat_dim = dim[1] * dim[2] * dim[3] # 4 * 4 * 64\n","flat = tf.reshape(pool7, [-1, flat_dim])\n","\n","###############################\n","# Fully Connected Layer 01 \n","\n","# for Final FC / (7 * 7 * 512, 4096) but (4 * 4 * 64, 256)\n","wfc1 = tf.Variable(tf.random_normal([flat_dim, 256], stddev=0.01))\n","bfc1 = tf.Variable(tf.random_normal([256]))\n","\n","# fc layer\n","fc1 = tf.add(tf.matmul(flat, wfc1), bfc1)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(fc1, [0])\n","fc1 = tf.nn.batch_normalization(fc1, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","fc1 = tf.nn.relu(fc1)\n","# dropout\n","fc1 = tf.nn.dropout(fc1, kp)\n","\n","###############################\n","# Fully Connected Layer 02\n","\n","# for Final FC / (4096, 4096) but (256, 256)\n","wfc2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))\n","bfc2 = tf.Variable(tf.random_normal([256]))\n","\n","# fc layer\n","fc2 = tf.add(tf.matmul(fc1, wfc2), bfc2)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(fc2, [0])\n","fc2 = tf.nn.batch_normalization(fc2, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","fc2 = tf.nn.relu(fc2)\n","# dropout\n","fc2 = tf.nn.dropout(fc2, kp)\n","\n","###############################\n","# Fully Connected Layer 03\n","\n","# for Final FC / (4096, 1000) but (256, 10)\n","wfc3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n","bfc3 = tf.Variable(tf.random_normal([10]))\n","\n","# fc layer\n","logits = tf.add(tf.matmul(fc2, wfc3), bfc3)\n","\n","#########################\n","# Cost & Optimizer\n","# Cost(loss) function & Optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y), name=\"cost\")\n","optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n","    \n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1), name=\"compare\")\n","acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q124hsIPVaru","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# Session initialize\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","###########################\n","# Traning\n","\n","print('Learning started. It takes sometime.')\n","\n","epochs = 20\n","batch_size = 256\n","learning_rate = 0.01\n","dropout = 0.7\n","n_of_batches = int(len(train_x) / batch_size)\n","\n","for epoch in range(epochs): \n","    print(\"%dth epoch\" % (epoch + 1))\n","\n","    for i in range(n_of_batches):\n","\n","        X_batch = train_x[(i * batch_size):((i + 1) * batch_size)]\n","        Y_batch = train_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","        sess.run(optimizer, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                       lr: learning_rate, kp: dropout})\n","\n","        # 학습 상황 디스플레이\n","        if ((i + 1) % 10 == 0): \n","            loss, accuracy = sess.run([cost, acc], \n","                                      feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                                 lr: learning_rate, kp: 1.0})\n","            print(\"%dth records, training cost: %.3f, accuracy: %.2f\" % ((i + 1) * batch_size, loss, accuracy * 100))\n","\n","print(\"Training Complete\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4lfCb2jqVarz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["###########################\n","# Evaluation\n","# accuracy = sess.run(acc, feed_dict={X: test_x, Y: np.eye(10)[test_y], lr: learning_rate, kp: 1.0})\n","# print('Accuracy: %.2f' % (accuracy * 100))\n","\n","accuracy = 0.0\n","n_of_batches = int(len(test_x) / batch_size)\n","for i in range(n_of_batches):\n","\n","    X_batch = test_x[(i * batch_size):((i + 1) * batch_size)]\n","    Y_batch = test_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","    accuracy += sess.run(acc, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], lr: learning_rate, kp: 1.0})\n","    \n","print('Accuracy: %.2f' % ((accuracy / n_of_batches) * 100))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Mge3PABGv22v","colab_type":"text"},"cell_type":"markdown","source":["## CIFAR-10 + GoogLeNet\n","\n","http://joelouismarino.github.io/blog_posts/blog_googlenet_keras.html\n","\n","![image](http://joelouismarino.github.io/images/blog_images/blog_googlenet_keras/googlenet_stem.png)\n","![image](http://joelouismarino.github.io/images/blog_images/blog_googlenet_keras/googlenet_auxiliary.png)\n","![image](http://joelouismarino.github.io/images/blog_images/blog_googlenet_keras/googlenet_output.png)\n","![image](https://cdn-images-1.medium.com/max/1600/1*66hY3zZTf0Lw2ItybiRxyg.png)\n"]},{"metadata":{"id":"o5xvAWF4v-em","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","\n","# Graph Clear\n","tf.reset_default_graph()\n","tf.set_random_seed(2017) # random seeding - reproduct\n","\n","############################\n","# Place Holders\n","lr = tf.placeholder(tf.float32) # learning rate\n","kp = tf.placeholder(tf.float32) # dropout\n","\n","X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"X\")\n","Y = tf.placeholder(tf.float32, [None, 10], name=\"Y\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X6JSemLHP8Dr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 1, 2\n","\n","# for Conv Layer 01 filter\n","w1 = tf.Variable(tf.random_normal([3, 3, 3, 16], stddev=0.01))\n","b1 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 32, 32, 16)\n","conv1 = tf.add(tf.nn.conv2d(X, w1, strides=[1, 1, 1, 1], padding='SAME'), b1)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv1, [0])\n","conv1 = tf.nn.batch_normalization(conv1, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv1 = tf.nn.relu(conv1)\n","\n","# LRN\n","# depth_radius=5, bias=1.0, alpha=1e-04, beta=0.75\n","norm1 = tf.nn.local_response_normalization(conv1, 5, 1.0, 1e-04, 0.75)\n","\n","# for Conv Layer 02 filter\n","w2 = tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=0.01))\n","b2 = tf.Variable(tf.random_normal([32], stddev=0.01))\n","\n","# Convolution Layer -> (?, 32, 32, 32)\n","conv2 = tf.add(tf.nn.conv2d(norm1, w2, strides=[1, 1, 1, 1], padding='SAME'), b2)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv2, [0])\n","conv2 = tf.nn.batch_normalization(conv2, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv2 = tf.nn.relu(conv2)\n","\n","# LRN\n","# depth_radius=5, bias=1.0, alpha=1e-04, beta=0.75\n","norm2 = tf.nn.local_response_normalization(conv2, 5, 1.0, 1e-04, 0.75)\n","\n","# Pooling Layer -> (?, 16, 16, 32)\n","pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_J98767TRLNT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["\n","#########################\n","# ConvLayer 3, 4\n","\n","# for Conv Layer 03-1 filter\n","w31 = tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=0.01))\n","b31 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv31 = tf.add(tf.nn.conv2d(pool2, w31, strides=[1, 1, 1, 1], padding='SAME'), b31)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv31, [0])\n","conv31 = tf.nn.batch_normalization(conv31, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv31 = tf.nn.relu(conv31)\n","\n","#-------------------------------\n","# for Conv Layer 03-2-1 filter\n","w321 = tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=0.01))\n","b321 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv321 = tf.add(tf.nn.conv2d(pool2, w321, strides=[1, 1, 1, 1], padding='SAME'), b321)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv321, [0])\n","conv321 = tf.nn.batch_normalization(conv321, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv321 = tf.nn.relu(conv321)\n","\n","# for Conv Layer 03-2-2 filter\n","w322 = tf.Variable(tf.random_normal([3, 3, 8, 8], stddev=0.01))\n","b322 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv322 = tf.add(tf.nn.conv2d(conv321, w322, strides=[1, 1, 1, 1], padding='SAME'), b322)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv322, [0])\n","conv32 = tf.nn.batch_normalization(conv322, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv32 = tf.nn.relu(conv32)\n","\n","#-------------------------------\n","# for Conv Layer 03-3-1 filter\n","w331 = tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=0.01))\n","b331 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv331 = tf.add(tf.nn.conv2d(pool2, w331, strides=[1, 1, 1, 1], padding='SAME'), b331)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv331, [0])\n","conv331 = tf.nn.batch_normalization(conv331, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv331 = tf.nn.relu(conv331)\n","\n","# for Conv Layer 03-3-2 filter\n","w332 = tf.Variable(tf.random_normal([5, 5, 8, 8], stddev=0.01))\n","b332 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer 02 -> (?, 16, 16, 8)\n","conv331 = tf.add(tf.nn.conv2d(conv331, w332, strides=[1, 1, 1, 1], padding='SAME'), b332)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv331, [0])\n","conv33 = tf.nn.batch_normalization(conv331, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv33 = tf.nn.relu(conv33)\n","\n","#-------------------------------\n","# for Conv Layer 03-4-1 filter\n","\n","# Pooling Layer -> (?, 16, 16, 32)\n","pool34 = tf.nn.max_pool(pool2, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')\n","\n","# for Conv Layer 03-4 filter\n","w34 = tf.Variable(tf.random_normal([1, 1, 32, 8], stddev=0.01))\n","b34 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv34 = tf.add(tf.nn.conv2d(pool34, w34, strides=[1, 1, 1, 1], padding='SAME'), b34)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv34, [0])\n","conv34 = tf.nn.batch_normalization(conv34, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv34 = tf.nn.relu(conv34)\n","\n","# concat => (?, 16, 16, 32)\n","conv3 = tf.concat([conv31, conv32, conv33, conv34], 3)\n","\n","# Pooling Layer -> (?, 8, 8, 32)\n","pool3 = tf.nn.avg_pool(conv3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kHw8w1YnwbdE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["##################################################\n","# Flatten layer & Fully Connected Layer\n","\n","###############################\n","# Flatten layer\n","dim = pool3.get_shape().as_list()\n","flat_dim = dim[1] * dim[2] * dim[3] # 8 * 8 * 32\n","flat = tf.reshape(pool3, [-1, flat_dim])\n","\n","###############################\n","# Fully Connected Layer 01\n","\n","# for Final FC\n","wfc1 = tf.Variable(tf.random_normal([flat_dim, 256], stddev=0.01))\n","bfc1 = tf.Variable(tf.random_normal([256]))\n","\n","# fc layer\n","fc1 = tf.add(tf.matmul(flat, wfc1), bfc1)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(fc1, [0])\n","fc1 = tf.nn.batch_normalization(fc1, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","fc1 = tf.nn.relu(fc1)\n","# dropout\n","fc1 = tf.nn.dropout(fc1, kp)\n","\n","###############################\n","# Fully Connected Layer 02\n","\n","# for Final FC \n","wfc2 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n","bfc2 = tf.Variable(tf.random_normal([10]))\n","\n","# fc layer\n","logits = tf.add(tf.matmul(fc1, wfc2), bfc2)\n","\n","#########################\n","# Cost & Optimizer\n","\n","# Cost(loss) function & Optimizer\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y), name=\"cost\")\n","optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n","    \n","# Test model and check accuracy\n","correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1), name=\"compare\")\n","acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IgS9c66E3VsG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# Session initialize\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","###########################\n","# Traning\n","\n","print('Learning started. It takes sometime.')\n","epochs = 20\n","batch_size = 256\n","learning_rate = 0.01\n","dropout = 0.7\n","n_of_batches = int(len(train_x) / batch_size)\n","\n","for epoch in range(epochs): \n","    print(\"%dth epoch\" % (epoch + 1))\n","\n","    for i in range(n_of_batches):\n","        X_batch = train_x[(i * batch_size):((i + 1) * batch_size)]\n","        Y_batch = train_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","        sess.run(optimizer, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                       lr: learning_rate, kp: dropout})\n","\n","        # 학습 상황 디스플레이\n","        if ((i + 1) % 10 == 0): \n","            loss, accuracy = sess.run([cost, acc], \n","                                      feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                                 lr: learning_rate, kp: 1.0})\n","            print(\"%dth records, training cost: %.3f, accuracy: %.2f\" % ((i + 1) * batch_size, loss, accuracy * 100))\n","\n","print(\"Training Complete\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r3QAVK2h3jsk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["###########################\n","# Evaluation\n","# accuracy = sess.run(acc, feed_dict={X: test_x, Y: np.eye(10)[test_y], lr: learning_rate, kp: 1.0})\n","# print('Accuracy: %.2f' % (accuracy * 100))\n","\n","accuracy = 0.0\n","n_of_batches = int(len(test_x) / batch_size)\n","for i in range(n_of_batches):\n","\n","    X_batch = test_x[(i * batch_size):((i + 1) * batch_size)]\n","    Y_batch = test_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","    accuracy += sess.run(acc, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], lr: learning_rate, kp: 1.0})\n","    \n","print('Accuracy: %.2f' % ((accuracy / n_of_batches) * 100))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FULCrLu0WmIh","colab_type":"text"},"cell_type":"markdown","source":["## CIFAR-10 + ResNet\n","\n","![image](https://cdn-images-1.medium.com/max/1314/1*S3TlG0XpQZSIpoDIUCQ0RQ.jpeg)\n","\n","![image](https://cdn-images-1.medium.com/max/1600/1*aq0q7gCvuNUqnMHh4cpnIw.png)\n","\n","![image](https://1.bp.blogspot.com/-J5pbMKnnmZ0/V9t_aeGiEnI/AAAAAAAAB6c/f8-B42TxWAwasaXYpzKyOzUT2DqgpfZ9QCEw/s1600/%25E1%2584%2591%25E1%2585%25A1%25E1%2584%258B%25E1%2585%25B5%25E1%2586%25AF%2B2016.%2B9.%2B16.%2B%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB%2B9%2B10%2B32.jpeg)\n","\n","https://m.blog.naver.com/PostView.nhn?blogId=laonple&logNo=220793640991&proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F\n"]},{"metadata":{"id":"lj0MQ3qq80Uv","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","\n","# Graph Clear\n","tf.reset_default_graph()\n","tf.set_random_seed(2017) # random seeding - reproduct\n","\n","############################\n","# Place Holders\n","lr = tf.placeholder(tf.float32) # learning rate\n","kp = tf.placeholder(tf.float32) # dropout\n","\n","X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"X\")\n","Y = tf.placeholder(tf.float32, [None, 10], name=\"Y\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"68BkOssVYWYZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 01\n","\n","# for Conv Layer 01 filter\n","w1 = tf.Variable(tf.random_normal([5, 5, 3, 16], stddev=0.01))\n","b1 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 32, 32, 16)\n","conv1 = tf.add(tf.nn.conv2d(X, w1, strides=[1, 1, 1, 1], padding='SAME'), b1)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv1, [0])\n","conv1 = tf.nn.batch_normalization(conv1, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv1 = tf.nn.relu(conv1)\n","\n","# Pooling Layer 01 -> (?, 16, 16, 16)\n","pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xnJcEG_pYsyX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 02\n","\n","# for Conv Layer 02-1 filter\n","w21 = tf.Variable(tf.random_normal([1, 1, 16, 8], stddev=0.01))\n","b21 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv21 = tf.add(tf.nn.conv2d(pool1, w21, strides=[1, 1, 1, 1], padding='SAME'), b21)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv21, [0])\n","conv21 = tf.nn.batch_normalization(conv21, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv21 = tf.nn.relu(conv21)\n","\n","# for Conv Layer 02-2 filter\n","w22 = tf.Variable(tf.random_normal([3, 3, 8, 8], stddev=0.01))\n","b22 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv22 = tf.add(tf.nn.conv2d(conv21, w22, strides=[1, 1, 1, 1], padding='SAME'), b22)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv22, [0])\n","conv22 = tf.nn.batch_normalization(conv22, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv22 = tf.nn.relu(conv22)\n","\n","# for Conv Layer 02-3 filter\n","w23 = tf.Variable(tf.random_normal([1, 1, 8, 16], stddev=0.01))\n","b23 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv23 = tf.add(tf.nn.conv2d(conv22, w23, strides=[1, 1, 1, 1], padding='SAME'), b23)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv23, [0])\n","conv23 = tf.nn.batch_normalization(conv23, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# merge with pool1\n","conv2 = tf.add(conv23, pool1)\n","\n","# relu -> (?, 16, 16, 16)\n","conv2 = tf.nn.relu(conv2)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"okYJb6HsbYNJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 03\n","\n","# for Conv Layer 03-1 filter\n","w31 = tf.Variable(tf.random_normal([1, 1, 16, 8], stddev=0.01))\n","b31 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv31 = tf.add(tf.nn.conv2d(conv2, w31, strides=[1, 1, 1, 1], padding='SAME'), b31)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv31, [0])\n","conv31 = tf.nn.batch_normalization(conv31, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv31 = tf.nn.relu(conv31)\n","\n","# for Conv Layer 03-2 filter\n","w32 = tf.Variable(tf.random_normal([3, 3, 8, 8], stddev=0.01))\n","b32 = tf.Variable(tf.random_normal([8], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 8)\n","conv32 = tf.add(tf.nn.conv2d(conv31, w32, strides=[1, 1, 1, 1], padding='SAME'), b32)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv32, [0])\n","conv32 = tf.nn.batch_normalization(conv32, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv32 = tf.nn.relu(conv32)\n","\n","# for Conv Layer 03-3 filter\n","w33 = tf.Variable(tf.random_normal([1, 1, 8, 16], stddev=0.01))\n","b33 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv33 = tf.add(tf.nn.conv2d(conv32, w33, strides=[1, 1, 1, 1], padding='SAME'), b33)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv33, [0])\n","conv33 = tf.nn.batch_normalization(conv33, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# merge with pool1\n","conv3 = tf.add(conv33, conv2)\n","\n","# relu -> (?, 16, 16, 16)\n","conv3 = tf.nn.relu(conv3)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vqr-WDqGdCif","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 04\n","\n","# for Conv Layer 04 filter\n","w41 = tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=0.01))\n","b41 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv41 = tf.add(tf.nn.conv2d(conv3, w41, strides=[1, 1, 1, 1], padding='SAME'), b41)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv41, [0])\n","conv41 = tf.nn.batch_normalization(conv41, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv41 = tf.nn.relu(conv41)\n","\n","# for Conv Layer 04 filter\n","w42 = tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=0.01))\n","b42 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv42 = tf.add(tf.nn.conv2d(conv41, w42, strides=[1, 1, 1, 1], padding='SAME'), b42)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv42, [0])\n","conv42 = tf.nn.batch_normalization(conv42, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","conv4 = tf.add(conv42, conv3)\n","\n","# relu\n","conv4 = tf.nn.relu(conv4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X83dqY0Ld2a5","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# ConvLayer 05\n","\n","# for Conv Layer 05 filter\n","w51 = tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=0.01))\n","b51 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv51 = tf.add(tf.nn.conv2d(conv4, w51, strides=[1, 1, 1, 1], padding='SAME'), b51)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv51, [0])\n","conv51 = tf.nn.batch_normalization(conv51, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","conv51 = tf.nn.relu(conv51)\n","\n","# for Conv Layer 04 filter\n","w52 = tf.Variable(tf.random_normal([3, 3, 16, 16], stddev=0.01))\n","b52 = tf.Variable(tf.random_normal([16], stddev=0.01))\n","\n","# Convolution Layer -> (?, 16, 16, 16)\n","conv52 = tf.add(tf.nn.conv2d(conv51, w52, strides=[1, 1, 1, 1], padding='SAME'), b52)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(conv52, [0])\n","conv52 = tf.nn.batch_normalization(conv52, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","conv5 = tf.add(conv52, conv4)\n","\n","# relu\n","conv5 = tf.nn.relu(conv5)\n","\n","# Pooling Layer -> (?, 8, 8, 16)\n","pool5 = tf.nn.max_pool(conv5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HbvH8lth9XDB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["##################################################\n","# Flatten layer & Fully Connected Layer\n","\n","###############################\n","# Flatten layer\n","dim = pool5.get_shape().as_list()\n","flat_dim = dim[1] * dim[2] * dim[3] # 8 * 8 * 16\n","flat = tf.reshape(pool5, [-1, flat_dim])\n","\n","###############################\n","# Fully Connected Layer 01\n","\n","# for Final FC\n","wfc1 = tf.Variable(tf.random_normal([flat_dim, 256], stddev=0.01))\n","bfc1 = tf.Variable(tf.random_normal([256]))\n","\n","# fc layer\n","fc1 = tf.add(tf.matmul(flat, wfc1), bfc1)\n","\n","# batch normalization\n","batch_mean, batch_var = tf.nn.moments(fc1, [0])\n","fc1 = tf.nn.batch_normalization(fc1, batch_mean, batch_var, None, None, 0.001) # epsilon = 0.001\n","\n","# relu\n","fc1 = tf.nn.relu(fc1)\n","# dropout\n","fc1 = tf.nn.dropout(fc1, kp)\n","\n","###############################\n","# Fully Connected Layer 02\n","\n","# for Final FC\n","wfc2 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n","bfc2 = tf.Variable(tf.random_normal([10]))\n","\n","# fc layer\n","logits = tf.add(tf.matmul(fc1, wfc2), bfc2)\n","\n","#########################\n","# Cost & Optimizer\n","\n","with tf.name_scope(\"Optimizer\"):\n","    # Cost(loss) function & Optimizer\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y), name=\"cost\")\n","    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n","    \n","with tf.name_scope(\"Prediction\"):\n","    # Test model and check accuracy\n","    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1), name=\"compare\")\n","    acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h5Vdo9uA9ZvV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#########################\n","# Session initialize\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","###########################\n","# Traning\n","\n","print('Learning started. It takes sometime.')\n","\n","epochs = 20\n","batch_size = 256\n","learning_rate = 0.01\n","dropout = 0.7\n","n_of_batches = int(len(train_x) / batch_size)\n","\n","for epoch in range(epochs): \n","    print(\"%dth epoch\" % (epoch + 1))\n","\n","    for i in range(n_of_batches):\n","\n","        X_batch = train_x[(i * batch_size):((i + 1) * batch_size)]\n","        Y_batch = train_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","        sess.run(optimizer, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                       lr: learning_rate, kp: dropout})\n","\n","        # 학습 상황 디스플레이\n","        if ((i + 1) % 10 == 0): \n","            loss, accuracy = sess.run([cost, acc], \n","                                      feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], \n","                                                 lr: learning_rate, kp: 1.0})\n","            print(\"%dth records, training cost: %.3f, accuracy: %.2f\" % ((i + 1) * batch_size, loss, accuracy * 100))\n","\n","print(\"Training Complete\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PDil5qNN9bgr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["###########################\n","# Evaluation\n","# accuracy = sess.run(acc, feed_dict={X: test_x, Y: np.eye(10)[test_y], lr: learning_rate, kp: 1.0})\n","# print('Accuracy: %.2f' % (accuracy * 100))\n","\n","accuracy = 0.0\n","n_of_batches = int(len(test_x) / batch_size)\n","for i in range(n_of_batches):\n","\n","    X_batch = test_x[(i * batch_size):((i + 1) * batch_size)]\n","    Y_batch = test_y[(i * batch_size):((i + 1) * batch_size)]\n","\n","    accuracy += sess.run(acc, feed_dict={X: X_batch, Y: np.eye(10)[Y_batch], lr: learning_rate, kp: 1.0})\n","    \n","print('Accuracy: %.2f' % ((accuracy / n_of_batches) * 100))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1D-o-Pn3ALri","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}